{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Wikipedia API...\n",
      "Loading KILT dataset...\n",
      "\n",
      "--- Debug: First sample structure ---\n",
      "{'id': '5a7a06935542990198eaf050', 'input': \"Which magazine was started first Arthur's Magazine or First for Women?\", 'meta': {'left_context': '', 'mention': '', 'right_context': '', 'partial_evidence': [], 'obj_surface': [], 'sub_surface': [], 'subj_aliases': [], 'template_questions': []}, 'output': [{'answer': \"Arthur's Magazine\", 'meta': {'score': -1}, 'provenance': [{'bleu_score': 1.0, 'start_character': 0, 'start_paragraph_id': 1, 'end_character': 112, 'end_paragraph_id': 1, 'meta': {'fever_page_id': '', 'fever_sentence_id': -1, 'annotation_id': '-1', 'yes_no_answer': '', 'evidence_span': []}, 'section': 'Section::::Abstract.', 'title': \"Arthur's Magazine\", 'wikipedia_id': '27290714'}, {'bleu_score': 1.0, 'start_character': 0, 'start_paragraph_id': 1, 'end_character': 80, 'end_paragraph_id': 1, 'meta': {'fever_page_id': '', 'fever_sentence_id': -1, 'annotation_id': '-1', 'yes_no_answer': '', 'evidence_span': []}, 'section': 'Section::::Abstract.', 'title': 'First for Women', 'wikipedia_id': '21232019'}]}]}\n",
      "-------------------------------------\n",
      "\n",
      "Found 0 unique Wikipedia articles to process.\n",
      "No unique titles found in the dataset. Exiting.\n",
      "\n",
      "Starting data generation for document structuring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Wikipedia Pages: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated 0 training pairs.\n",
      "Saving data to 'document_structuring_data.jsonl'...\n",
      "Stage 2 data generation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wikipediaapi\n",
    "import itertools\n",
    "\n",
    "# --- 1. 初始化工具 (无变化) ---\n",
    "print(\"Initializing Wikipedia API...\")\n",
    "wiki_api = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    user_agent=\"MyKILTDataGenerator/1.0 (contact@example.com)\"\n",
    ")\n",
    "\n",
    "# --- 2. 辅助函数 (无变化) ---\n",
    "def clean_text(text):\n",
    "    \"\"\"移除维基百科文本中的常见噪声\"\"\"\n",
    "    # ... (代码与之前相同) ...\n",
    "    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "    text = re.sub(r'\\[\\[File:.*?\\]\\]', '', text)\n",
    "    text = re.sub(r\"''+\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def truncate_paragraph(paragraph_text, k=30):\n",
    "    \"\"\"对段落进行截断\"\"\"\n",
    "    # ... (代码与之前相同) ...\n",
    "    words = paragraph_text.split()\n",
    "    if len(words) > 2 * k:\n",
    "        return ' '.join(words[:k]) + ' <skip> ' + ' '.join(words[-k:])\n",
    "    else:\n",
    "        return paragraph_text\n",
    "\n",
    "def process_wiki_section(section, level=2):\n",
    "    \"\"\"递归处理维基百科页面章节\"\"\"\n",
    "    # ... (代码与之前相同) ...\n",
    "    plain_texts = []\n",
    "    xml_parts = []\n",
    "    paragraphs = [p for p in section.text.split('\\n') if p.strip()]\n",
    "    for p_text in paragraphs:\n",
    "        cleaned_p = clean_text(p_text)\n",
    "        if len(cleaned_p.split()) < 5:\n",
    "            continue\n",
    "        plain_texts.append(cleaned_p)\n",
    "        truncated_p = truncate_paragraph(cleaned_p)\n",
    "        xml_parts.append(truncated_p + \"<br>\")\n",
    "    for subsection in section.sections:\n",
    "        sub_plain, sub_xml = process_wiki_section(subsection, level + 1)\n",
    "        tag = \"subsection\" if level >= 2 else \"section\"\n",
    "        title = clean_text(subsection.title)\n",
    "        plain_texts.append(title)\n",
    "        plain_texts.extend(sub_plain)\n",
    "        xml_parts.append(f\"<{tag}: {title}>\")\n",
    "        xml_parts.extend(sub_xml)\n",
    "        xml_parts.append(f\"</{tag.split(':')[0]}>\")\n",
    "    return plain_texts, xml_parts\n",
    "\n",
    "def generate_training_pair(wiki_page):\n",
    "    \"\"\"为单个维基百科页面生成训练数据对\"\"\"\n",
    "    # ... (代码与之前相同) ...\n",
    "    if not wiki_page.exists():\n",
    "        return None, None\n",
    "    summary_text = clean_text(wiki_page.summary)\n",
    "    if not summary_text:\n",
    "        return None, None\n",
    "    plain_texts = [summary_text]\n",
    "    xml_parts = [f\"<abstract>{truncate_paragraph(summary_text)}</abstract>\"]\n",
    "    for section in wiki_page.sections:\n",
    "        sec_plain, sec_xml = process_wiki_section(section)\n",
    "        title = clean_text(section.title)\n",
    "        plain_texts.append(title)\n",
    "        plain_texts.extend(sec_plain)\n",
    "        xml_parts.append(f\"<section: {title}>\")\n",
    "        xml_parts.extend(sec_xml)\n",
    "        xml_parts.append(\"</section>\")\n",
    "    input_text = \"\\n\".join(plain_texts)\n",
    "    output_xml = re.sub(r'(\\s*<br>\\s*)+', '<br>', \"\".join(xml_parts)).strip()\n",
    "    if len(input_text.split()) < 50 or len(output_xml.split()) < 20:\n",
    "        return None, None\n",
    "    return input_text, output_xml\n",
    "\n",
    "# --- 4. 主循环和数据加载/保存 (已修复和优化) ---\n",
    "\n",
    "def extract_titles_from_kilt(dataset):\n",
    "    \"\"\"\n",
    "    一个更健壮的函数，用于从KILT数据集中提取维基百科标题。\n",
    "    \"\"\"\n",
    "    all_titles = []\n",
    "    \n",
    "    # KILT数据集的每个样本都是一个独立的字典\n",
    "    for sample in dataset:\n",
    "        # 'output' 键的值可能是一个字典或列表\n",
    "        output_field = sample.get('output')\n",
    "        if not output_field:\n",
    "            continue\n",
    "\n",
    "        # 确保我们处理的是一个列表，即使它只有一个元素\n",
    "        outputs_to_check = output_field if isinstance(output_field, list) else [output_field]\n",
    "\n",
    "        for item in outputs_to_check:\n",
    "            # 检查 'provenance' 键\n",
    "            if isinstance(item, dict) and 'provenance' in item:\n",
    "                provenance_list = item['provenance']\n",
    "                \n",
    "                # 再次确保 provenance_list 是一个列表\n",
    "                if isinstance(provenance_list, list):\n",
    "                    for prov in provenance_list:\n",
    "                        if isinstance(prov, dict) and 'wikipedia_title' in prov:\n",
    "                            all_titles.append(prov['wikipedia_title'])\n",
    "                            \n",
    "    # 去重并排序\n",
    "    return sorted(list(set(all_titles)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading KILT dataset...\")\n",
    "    unique_titles = []\n",
    "    \n",
    "    try:\n",
    "        # 使用 kilt_tasks 的 hotpotqa 子集作为示例\n",
    "        kilt_dataset = load_dataset(\"kilt_tasks\", \"hotpotqa\", split=\"train[:500]\")\n",
    "        \n",
    "        # 调试: 打印第一个样本的结构\n",
    "        if len(kilt_dataset) > 0:\n",
    "            print(\"\\n--- Debug: First sample structure ---\")\n",
    "            print(kilt_dataset[0])\n",
    "            print(\"-------------------------------------\\n\")\n",
    "            \n",
    "        # 使用新的健壮函数来提取标题\n",
    "        unique_titles = extract_titles_from_kilt(kilt_dataset)\n",
    "        \n",
    "        print(f\"Found {len(unique_titles)} unique Wikipedia articles to process.\")\n",
    "        if len(unique_titles) > 0:\n",
    "            print(\"Sample titles:\", unique_titles[:5])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load and parse KILT dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    if not unique_titles:\n",
    "        print(\"No unique titles found in the dataset. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    training_data = []\n",
    "    output_filename = \"document_structuring_data.jsonl\"\n",
    "\n",
    "    print(\"\\nStarting data generation for document structuring...\")\n",
    "    for title in tqdm(unique_titles, desc=\"Processing Wikipedia Pages\"):\n",
    "        try:\n",
    "            page = wiki_api.page(title)\n",
    "            input_text, output_xml = generate_training_pair(page)\n",
    "            if input_text and output_xml:\n",
    "                training_data.append({\n",
    "                    \"input_text\": input_text,\n",
    "                    \"output_xml\": output_xml\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping article '{title}' due to an unexpected error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nSuccessfully generated {len(training_data)} training pairs.\")\n",
    "\n",
    "    print(f\"Saving data to '{output_filename}'...\")\n",
    "    try:\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in training_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        print(\"Stage 2 data generation complete!\")\n",
    "        if training_data:\n",
    "            print(\"\\n--- Sample Generated Data ---\")\n",
    "            print(json.dumps(training_data[0], indent=2, ensure_ascii=False))\n",
    "            print(\"---------------------------\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Wikipedia API...\n",
      "Loading KILT dataset...\n",
      "Found 769 unique Wikipedia articles to process.\n",
      "Sample titles: ['101 Park Avenue', '12th Lumières Awards', '1895–96 Everton F.C. season', '1963 Pan American Games', '1976 German Grand Prix', '1979 (song)', \"1999 French Open – Women's Doubles\", '2000–01 Utah Jazz season', '2001 Asian Junior Athletics Championships', '2003–04 VfL Wolfsburg season']\n",
      "\n",
      "Starting data generation for document structuring...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Wikipedia Pages: 100%|██████████| 769/769 [11:51<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated 751 training pairs.\n",
      "Saving data to 'document_structuring_data.jsonl'...\n",
      "Stage 2 data generation complete!\n",
      "\n",
      "--- Sample Generated Data ---\n",
      "{\n",
      "  \"input_text\": \"101 Park Avenue is a 629-foot (192 m) tall skyscraper at 41st Street and Park Avenue in the Murray Hill neighborhood of Manhattan, New York.\\nEli Attia Architects designed the tower. The building contains various tenants, as well as several attractions and amenities such as Convene, Five Iron Golf, and Museum of the Dog. \\nThe building is assigned its own ZIP Code, 10178; it was one of 41 buildings in Manhattan that had their own ZIP Codes as of 2019.\\nIn popular culture\\nIt features in the 1983 film Escape from the Bronx as President Clark's headquarters of the General Construction (GC) Corporation. It was used as the facade of the fictional \\\"Pemrose building\\\" in the 1987 film The Secret of My Success, as well as the fictional \\\"Clamp Center\\\" in the 1990 film Gremlins 2: The New Batch. The building features in the 1991 Jeff Bridges film The Fisher King, and is shown as the site of George Costanza's office in a few ninth-season episodes of Seinfeld, as well as Dudley Moore's character's office in the film Crazy People. Person of Interest used the building several times including as IFT Headquarters in season one. The rooftop of the building was featured in the 2011 film Friends with Benefits. The building is featured as a crash site in the 2012 film The Avengers.  It was used for a brief exit shot in the 2012 Richard Gere film Arbitrage. Most recently, it was featured as the office in the 2019 film Isn’t It Romantic. The building's roof was used in the 1985 second season feature length opener of Miami Vice, where Crockett & Tubbs face a showdown with an NYPD hostile to their investigation into a powerful Colombian drug dealing syndicate operating in the city.\\nIt is also on the roof of this skyscraper that the final shots of the clip made for the piece Looking up, by Michel Petrucciani, were shot.\\nNotable tenants\\nCurtis, Mallet-Prevost, Colt & Mosle\\nTata Consultancy Services (North American HQ)\\nHJ Kalikow & Co LLC\\nSee also\\nList of tallest buildings in New York City\\nReferences\\nExternal links\",\n",
      "  \"output_xml\": \"<abstract>101 Park Avenue is a 629-foot (192 m) tall skyscraper at 41st Street and Park <skip> one of 41 buildings in Manhattan that had their own ZIP Codes as of 2019.</abstract><section: In popular culture>It features in the 1983 film Escape from the Bronx as President Clark's headquarters of <skip> hostile to their investigation into a powerful Colombian drug dealing syndicate operating in the city.<br>It is also on the roof of this skyscraper that the final shots of the clip made for the piece Looking up, by Michel Petrucciani, were shot.<br></section><section: Notable tenants>Curtis, Mallet-Prevost, Colt & Mosle<br>Tata Consultancy Services (North American HQ)<br>HJ Kalikow & Co LLC<br></section><section: See also>List of tallest buildings in New York City<br></section><section: References></section><section: External links></section>\"\n",
      "}\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wikipediaapi\n",
    "import itertools\n",
    "\n",
    "\n",
    "print(\"Initializing Wikipedia API...\")\n",
    "wiki_api = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    user_agent=\"MyKILTDataGenerator/1.0 (contact@example.com)\"\n",
    ")\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "    text = re.sub(r'\\[\\[File:.*?\\]\\]', '', text)\n",
    "    text = re.sub(r\"''+\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def truncate_paragraph(paragraph_text, k=15):\n",
    "    words = paragraph_text.split()\n",
    "    if len(words) > 2 * k:\n",
    "        return ' '.join(words[:k]) + ' <skip> ' + ' '.join(words[-k:])\n",
    "    else:\n",
    "        return paragraph_text\n",
    "\n",
    "def process_wiki_section(section, level=2):\n",
    "    plain_texts = []\n",
    "    xml_parts = []\n",
    "    paragraphs = [p for p in section.text.split('\\n') if p.strip()]\n",
    "    for p_text in paragraphs:\n",
    "        cleaned_p = clean_text(p_text)\n",
    "        if len(cleaned_p.split()) < 5:\n",
    "            continue\n",
    "        plain_texts.append(cleaned_p)\n",
    "        truncated_p = truncate_paragraph(cleaned_p)\n",
    "        xml_parts.append(truncated_p + \"<br>\")\n",
    "    for subsection in section.sections:\n",
    "        sub_plain, sub_xml = process_wiki_section(subsection, level + 1)\n",
    "        tag = \"subsection\" if level >= 2 else \"section\"\n",
    "        title = clean_text(subsection.title)\n",
    "        plain_texts.append(title)\n",
    "        plain_texts.extend(sub_plain)\n",
    "        xml_parts.append(f\"<{tag}: {title}>\")\n",
    "        xml_parts.extend(sub_xml)\n",
    "        xml_parts.append(f\"</{tag.split(':')[0]}>\")\n",
    "    return plain_texts, xml_parts\n",
    "\n",
    "def generate_training_pair(wiki_page):\n",
    "    if not wiki_page.exists():\n",
    "        return None, None\n",
    "    summary_text = clean_text(wiki_page.summary)\n",
    "    if not summary_text:\n",
    "        return None, None\n",
    "    plain_texts = [summary_text]\n",
    "    xml_parts = [f\"<abstract>{truncate_paragraph(summary_text)}</abstract>\"]\n",
    "    for section in wiki_page.sections:\n",
    "        sec_plain, sec_xml = process_wiki_section(section)\n",
    "        title = clean_text(section.title)\n",
    "        plain_texts.append(title)\n",
    "        plain_texts.extend(sec_plain)\n",
    "        xml_parts.append(f\"<section: {title}>\")\n",
    "        xml_parts.extend(sec_xml)\n",
    "        xml_parts.append(\"</section>\")\n",
    "    input_text = \"\\n\".join(plain_texts)\n",
    "    output_xml = re.sub(r'(\\s*<br>\\s*)+', '<br>', \"\".join(xml_parts)).strip()\n",
    "    if len(input_text.split()) < 50 or len(output_xml.split()) < 20:\n",
    "        return None, None\n",
    "    return input_text, output_xml\n",
    "\n",
    "\n",
    "\n",
    "def extract_titles_from_kilt(dataset):\n",
    "    \"\"\"\n",
    "    一个严格按照KILT HotpotQA样本结构来提取标题的函数。\n",
    "    \"\"\"\n",
    "    all_titles = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # 'output' 键的值是一个列表\n",
    "        output_list = sample.get('output')\n",
    "        if not isinstance(output_list, list):\n",
    "            continue\n",
    "\n",
    "        for output_item in output_list:\n",
    "            # 每个元素是一个字典，包含 'provenance'\n",
    "            if not isinstance(output_item, dict):\n",
    "                continue\n",
    "            \n",
    "            provenance_list = output_item.get('provenance')\n",
    "            if not isinstance(provenance_list, list):\n",
    "                continue\n",
    "\n",
    "            for prov_item in provenance_list:\n",
    "                if not isinstance(prov_item, dict):\n",
    "                    continue\n",
    "                \n",
    "                # KILT数据集的键名可能是 'title' 或 'wikipedia_title'\n",
    "                # 我们两个都检查，以提高兼容性\n",
    "                title = prov_item.get('title') or prov_item.get('wikipedia_title')\n",
    "                \n",
    "                if title:\n",
    "                    all_titles.append(title)\n",
    "                            \n",
    "    # 去重并排序\n",
    "    return sorted(list(set(all_titles)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading KILT dataset...\")\n",
    "    unique_titles = []\n",
    "    \n",
    "    try:\n",
    "        kilt_dataset = load_dataset(\"kilt_tasks\", \"hotpotqa\", split=\"train[500:1500]\")\n",
    "        \n",
    "        \n",
    "        unique_titles = extract_titles_from_kilt(kilt_dataset)\n",
    "        \n",
    "        print(f\"Found {len(unique_titles)} unique Wikipedia articles to process.\")\n",
    "        if len(unique_titles) > 0:\n",
    "            print(\"Sample titles:\", unique_titles[:10]) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during dataset loading or parsing: {e}\")\n",
    "        exit()\n",
    "\n",
    "    if not unique_titles:\n",
    "        print(\"No unique titles found in the dataset. This might be normal for a small sample. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    training_data = []\n",
    "    output_filename = \"document_structuring_data.jsonl\"\n",
    "\n",
    "    print(\"\\nStarting data generation for document structuring...\")\n",
    "    for title in tqdm(unique_titles, desc=\"Processing Wikipedia Pages\"):\n",
    "        try:\n",
    "            page = wiki_api.page(title)\n",
    "            input_text, output_xml = generate_training_pair(page)\n",
    "            if input_text and output_xml:\n",
    "                training_data.append({\n",
    "                    \"input_text\": input_text,\n",
    "                    \"output_xml\": output_xml\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping article '{title}' due to an unexpected error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nSuccessfully generated {len(training_data)} training pairs.\")\n",
    "\n",
    "    print(f\"Saving data to '{output_filename}'...\")\n",
    "    try:\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in training_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        print(\"Stage 2 data generation complete!\")\n",
    "        if training_data:\n",
    "            print(\"\\n--- Sample Generated Data ---\")\n",
    "            \n",
    "            if training_data:\n",
    "                print(json.dumps(training_data[0], indent=2, ensure_ascii=False))\n",
    "            print(\"---------------------------\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入 JSONL 文件\n",
    "with open(\"step2_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in training_data:\n",
    "        json_line = json.dumps(item, ensure_ascii=False)\n",
    "        f.write(json_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenAI and Wikipedia APIs...\n",
      "Loading KILT dataset...\n",
      "Starting Stage 3 data generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Samples: 100%|██████████| 200/200 [06:18<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated 159 training samples.\n",
      "Saving data to 'global_selection_data3.jsonl'...\n",
      "Stage 3 data generation complete!\n",
      "\n",
      "--- Sample Generated Data ---\n",
      "{\n",
      "  \"input\": \"Question: What piece was composed in 1937 for the American Bandmasters Association and has a melody similar to Green Bushes?\\nAbstract: Green Bushes is an English folk song (Roud #1040, Laws P2) which is featured in the second movement of Vaughan Williams's English Folk Song Suite, in Percy Grainger's Green Bushes (Passacaglia on an English Folksong), and in George Butterworth's The Banks of Green Willow. The melody is very similar to that of the \\\"Lost Lady Found\\\" movement of Percy Grainger's Lincolnshire Posy, and to \\\"Cutty Wren\\\".\\nAccording to Roud and Bishop\\n\\nThis was an immensely popular song, collected many times across England, although not so often elsewhere. It was also very popular with nineteenth-century broadside printers.\\nThe song first appears in broadsides of the 1820s or 1830s. Its popularity was hugely increased by a popular melodrama The Green Bushes, or A Hundred Years Ago by John Baldwin Buckstone, first performed in 1845. The heroine of the play made repeated reference to the song and sang a few verses, with the result that the sheet music was published soon after.\\nOutline:\\nRecordings\\nLyrics\\nReferences\\nExternal links\",\n",
      "  \"output\": [\n",
      "    \"abstract\",\n",
      "    \"References\"\n",
      "  ]\n",
      "}\n",
      "---------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import wikipediaapi\n",
    "\n",
    "# --- 1. 初始化工具 ---\n",
    "\n",
    "print(\"Initializing OpenAI and Wikipedia APIs...\")\n",
    "# 初始化你的LLM Client\n",
    "try:\n",
    "    client = OpenAI(\n",
    "        api_key=\"sk-5b02f81c8ebb45db837eb74acc16437e\",\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "    )\n",
    "    if not client.api_key or client.api_key == \"YOUR_DASHSCOPE_API_KEY\":\n",
    "        raise ValueError(\"API Key not found. Please set the environment variable.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 初始化维基百科API\n",
    "wiki_api = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
    "    user_agent=\"MyStage3DataGenerator/1.0 (contact@example.com)\"\n",
    ")\n",
    "\n",
    "# --- 2. 辅助函数 ---\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\{\\{.*?\\}\\}', '', text)\n",
    "    text = re.sub(r'\\[\\[File:.*?\\]\\]', '', text)\n",
    "    text = re.sub(r\"''+\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_document_outline(wiki_page):\n",
    "    \"\"\"从维基百科页面对象中解析出摘要和大纲。\"\"\"\n",
    "    if not wiki_page.exists():\n",
    "        return None, None\n",
    "    \n",
    "    # 摘要是页面开头的第一段非空文本\n",
    "    abstract = clean_text(wiki_page.summary)\n",
    "    \n",
    "    # 大纲是所有章节的标题列表\n",
    "    outline = []\n",
    "    for section in wiki_page.sections:\n",
    "        outline.append(clean_text(section.title))\n",
    "        # 也可以包含子章节，但为简化起见，我们先只用顶级章节\n",
    "        for subsection in section.sections:\n",
    "            outline.append(f\"  - {clean_text(subsection.title)}\")\n",
    "            \n",
    "    if not abstract or not outline:\n",
    "        return None, None\n",
    "        \n",
    "    return abstract, outline\n",
    "\n",
    "def create_prompt_for_global_selection(query, abstract, outline):\n",
    "    \"\"\"根据论文附录 Prompt B 构建标注指令。\"\"\"\n",
    "    outline_str = \"\\n\".join(f\"- {title}\" for title in outline)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant. Your task is to identify which sections of a document are relevant for answering a given question.\n",
    "\n",
    "You will be provided with three inputs:\n",
    "1. A question.\n",
    "2. The abstract of a document.\n",
    "3. An outline of the document, containing its section titles.\n",
    "\n",
    "Based on these inputs, select all the section titles from the outline that are helpful for answering the question. Always include 'abstract' in your selection if the abstract itself contains relevant information.\n",
    "\n",
    "**Output Format:**\n",
    "Provide the output as a JSON object with a single key \"selected_titles\", which is a list of strings. Each string must be an exact title from the outline, or the word 'abstract'.\n",
    "\n",
    "**Example:**\n",
    "Question: Who was the first man to walk on the moon, and what was his role in the Apollo program?\n",
    "Document abstract: Neil Armstrong was an American astronaut and the first person to walk on the Moon. He was a key figure in the Apollo 11 mission...\n",
    "Document outline:\n",
    "- Early life\n",
    "- NASA career\n",
    "- Apollo 11 mission\n",
    "- Later life\n",
    "- Legacy\n",
    "Output:\n",
    "{{\n",
    "  \"selected_titles\": [\"abstract\", \"NASA career\", \"Apollo 11 mission\"]\n",
    "}}\n",
    "\n",
    "**Your Task:**\n",
    "Question: {query}\n",
    "Document abstract: {abstract}\n",
    "Document outline:\n",
    "{outline_str}\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def get_global_selection_from_llm(query, abstract, outline):\n",
    "    \"\"\"调用 LLM API 获取相关的章节标题。\"\"\"\n",
    "    prompt = create_prompt_for_global_selection(query, abstract, outline)\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen3-30b-a3b\", # 使用你的模型\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"}, # 尝试使用JSON模式\n",
    "            extra_body={\"enable_thinking\": False}\n",
    "        )\n",
    "        response_content = completion.choices[0].message.content\n",
    "        result_json = json.loads(response_content)\n",
    "        \n",
    "        # 验证返回的数据结构\n",
    "        selected_titles = result_json.get(\"selected_titles\")\n",
    "        if isinstance(selected_titles, list):\n",
    "            return selected_titles\n",
    "        else:\n",
    "            print(f\"Warning: LLM returned invalid format for query '{query}'. Response: {response_content}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM for query '{query}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 3. 主处理流程 ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading KILT dataset...\")\n",
    "    try:\n",
    "        # 加载少量数据作为示例\n",
    "        kilt_dataset = load_dataset(\"kilt_tasks\", \"hotpotqa\", split=\"train[600:800]\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load KILT dataset: {e}\")\n",
    "        exit()\n",
    "\n",
    "    training_data = []\n",
    "    output_filename = \"global_selection_data3.jsonl\"\n",
    "    \n",
    "    print(\"Starting Stage 3 data generation...\")\n",
    "    for sample in tqdm(kilt_dataset, desc=\"Processing Samples\"):\n",
    "        query = sample['input']\n",
    "        \n",
    "        # 一个查询可能关联多个文档，我们只处理第一个\n",
    "        provenance_list = sample['output'][0].get('provenance')\n",
    "        if not provenance_list:\n",
    "            continue\n",
    "        \n",
    "        # 以第一个证据来源作为文档\n",
    "        doc_title = provenance_list[0].get('title') or provenance_list[0].get('wikipedia_title')\n",
    "        if not doc_title:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # 获取文档大纲\n",
    "            wiki_page = wiki_api.page(doc_title)\n",
    "            abstract, outline = get_document_outline(wiki_page)\n",
    "            if not abstract or not outline:\n",
    "                # print(f\"Skipping '{doc_title}' due to missing abstract or outline.\")\n",
    "                continue\n",
    "\n",
    "            # 调用LLM进行标注\n",
    "            selected_titles = get_global_selection_from_llm(query, abstract, outline)\n",
    "            \n",
    "            if selected_titles is not None:\n",
    "                # 构造最终的训练样本\n",
    "                input_prompt = f\"Question: {query}\\nAbstract: {abstract}\\nOutline:\\n\" + \"\\n\".join(outline)\n",
    "                training_data.append({\n",
    "                    \"input\": input_prompt,\n",
    "                    \"output\": selected_titles\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred for doc '{doc_title}': {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nSuccessfully generated {len(training_data)} training samples.\")\n",
    "\n",
    "    print(f\"Saving data to '{output_filename}'...\")\n",
    "    try:\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            for item in training_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        print(\"Stage 3 data generation complete!\")\n",
    "        if training_data:\n",
    "            print(\"\\n--- Sample Generated Data ---\")\n",
    "            print(json.dumps(training_data[0], indent=2, ensure_ascii=False))\n",
    "            print(\"---------------------------\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}